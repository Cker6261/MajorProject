# ğŸ“ LungXAI Project Review Guide
## Complete Documentation for Project Defense & Viva

---

# TABLE OF CONTENTS

1. [Project Overview](#1-project-overview)
2. [Problem Statement & Motivation](#2-problem-statement--motivation)
3. [Complete Architecture](#3-complete-architecture)
4. [Technology Stack - What, Where & Why](#4-technology-stack---what-where--why)
5. [Module-by-Module Breakdown](#5-module-by-module-breakdown)
6. [Key Algorithms & Techniques](#6-key-algorithms--techniques)
7. [Dataset Details](#7-dataset-details)
8. [Training Process](#8-training-process)
9. [Evaluation Metrics](#9-evaluation-metrics)
10. [Potential Questions & Answers](#10-potential-questions--answers)
11. [Quick Reference Cards](#11-quick-reference-cards)

---

# 1. PROJECT OVERVIEW

## 1.1 Project Title
**"Explainable AI for Multi-Class Lung Cancer Classification Using Deep Learning and RAG-Based Knowledge Retrieval"**

## 1.2 One-Line Summary
A deep learning system that not only classifies lung CT scan images into cancer types but also provides **visual explanations** (WHERE the model looks) and **textual explanations** (WHY those regions matter) using Grad-CAM and RAG.

## 1.3 Key Innovation
| Traditional AI | Our System |
|---------------|------------|
| Only predicts class | Predicts + Explains |
| Black-box | Transparent |
| No medical context | Provides medical knowledge |
| Single output | Visual + Textual explanation |

## 1.4 Classes Classified (5 Classes)
| Class | Description | Typical Location |
|-------|-------------|------------------|
| **Adenocarcinoma** | Most common lung cancer (~40%) | Peripheral/Outer regions |
| **Squamous Cell Carcinoma** | Second most common (~30%) | Central/Hilar regions |
| **Large Cell Carcinoma** | Fast-growing, aggressive | Peripheral, larger masses |
| **Benign Cases** | Non-cancerous lesions | Variable |
| **Normal Cases** | Healthy lung tissue | Clear patterns |

---

# 2. PROBLEM STATEMENT & MOTIVATION

## 2.1 The Problem
1. **Lung cancer is the leading cause of cancer deaths worldwide**
2. **Early detection is critical** - 5-year survival rate is 56% if detected early vs 5% if detected late
3. **Radiologist shortage** - Increasing CT scan volumes with limited experts
4. **AI trust issue** - Doctors don't trust black-box AI predictions

## 2.2 Why Explainability Matters
```
âŒ Without Explainability:
   AI says: "Adenocarcinoma (92%)"
   Doctor thinks: "But why? I can't trust this."

âœ… With Explainability:
   AI says: "Adenocarcinoma (92%)"
   + Shows: Heatmap highlighting peripheral region
   + Explains: "Adenocarcinoma typically presents in peripheral 
               regions. Model focused on upper-right peripheral 
               area with ground-glass opacity pattern."
   Doctor thinks: "This aligns with clinical knowledge. I can verify."
```

## 2.3 Project Objectives
1. **Classify** lung CT images into 5 categories with high accuracy
2. **Visualize** model attention using Grad-CAM
3. **Explain** predictions using medical knowledge (RAG approach)
4. **Bridge** the gap between AI predictions and clinical understanding

---

# 3. COMPLETE ARCHITECTURE

## 3.1 High-Level Pipeline
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EXPLAINABLE LUNG CANCER CLASSIFICATION              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: CT Scan Image (PNG/JPG)                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 1. PREPROCESSING  â”‚               â”‚ 2. CLASSIFICATION â”‚
        â”‚    - Resize 224x224               â”‚    - ResNet-50    â”‚
        â”‚    - Normalize    â”‚               â”‚    - Transfer     â”‚
        â”‚    - ToTensor     â”‚               â”‚      Learning     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                                   â”‚
                    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚           â”‚                       â”‚
                    â–¼           â–¼                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     3. GRAD-CAM GENERATION    â”‚   â”‚  Prediction +     â”‚
        â”‚     - Hook last conv layer    â”‚   â”‚  Confidence       â”‚
        â”‚     - Compute gradients       â”‚   â”‚  (e.g., Adeno 92%)â”‚
        â”‚     - Generate heatmap        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                    â”‚                                 â”‚
                    â–¼                                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
        â”‚  4. XAI-TO-TEXT CONVERSION    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚  - Analyze heatmap spatially  â”‚
        â”‚  - Identify focus regions     â”‚
        â”‚  - Generate keywords          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  5. RAG KNOWLEDGE RETRIEVAL   â”‚
        â”‚  - Query knowledge base       â”‚
        â”‚  - Retrieve medical facts     â”‚
        â”‚  - Get citations/sources      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT:                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Original Image  â”‚  â”‚ Grad-CAM Heatmapâ”‚  â”‚ Textual Explanation         â”‚  â”‚
â”‚  â”‚                 â”‚  â”‚  (Where model   â”‚  â”‚ - Visual evidence           â”‚  â”‚
â”‚  â”‚                 â”‚  â”‚   focused)      â”‚  â”‚ - Medical context           â”‚  â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚ - Cited sources             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3.2 Data Flow Diagram
```
CT Image â†’ Preprocessing â†’ Model Forward Pass â†’ Prediction
                              â†“
                        Feature Maps (layer4)
                              â†“
                        Backward Pass â†’ Gradients
                              â†“
                        Grad-CAM Heatmap
                              â†“
                        Spatial Analysis â†’ Keywords
                              â†“
                        Knowledge Retrieval â†’ Medical Facts
                              â†“
                        Combined Explanation
```

---

# 4. TECHNOLOGY STACK - WHAT, WHERE & WHY

## 4.1 Core Framework

| Technology | What Is It | Where Used | Why Used |
|------------|------------|------------|----------|
| **Python 3.10+** | Programming language | Entire project | Industry standard for ML, extensive libraries |
| **PyTorch 2.0+** | Deep learning framework | Model, training, inference | Academic flexibility, dynamic graphs, debugging ease |
| **torchvision** | Vision library | Pretrained models, transforms | Official PyTorch extension for computer vision |

## 4.2 Model Architecture

| Technology | What Is It | Where Used | Why Used |
|------------|------------|------------|----------|
| **ResNet-50** | 50-layer residual network | `classifier.py` | Proven performance, residual connections prevent vanishing gradients, Grad-CAM compatible |
| **Transfer Learning** | Reusing pretrained weights | Model initialization | ImageNet pretrained weights capture general features, compensates for limited medical data |
| **Dropout (0.5)** | Regularization technique | Final FC layer | Prevents overfitting, improves generalization |

### Why ResNet-50 Specifically?
```
ResNet-50 Benefits:
â”œâ”€â”€ âœ… 50 layers deep - captures complex patterns
â”œâ”€â”€ âœ… Residual connections - solves vanishing gradient
â”œâ”€â”€ âœ… 2048 features before FC - rich representations
â”œâ”€â”€ âœ… Well-documented - easy to explain in viva
â”œâ”€â”€ âœ… Grad-CAM compatible - layer4 gives clear heatmaps
â””â”€â”€ âœ… ImageNet pretrained - 1000-class general features

Alternative Considered:
â”œâ”€â”€ VGG-16: Simpler but fewer features, no residuals
â”œâ”€â”€ EfficientNet: More efficient but harder to explain
â””â”€â”€ ViT: Newer but attention mechanism different from Grad-CAM
```

## 4.3 Explainability Stack

| Technology | What Is It | Where Used | Why Used |
|------------|------------|------------|----------|
| **Grad-CAM** | Gradient-weighted Class Activation Mapping | `gradcam.py` | Visual explanation, shows important regions, no model modification needed |
| **OpenCV** | Computer vision library | `visualize.py` | Heatmap overlay, colormap application |
| **Matplotlib** | Plotting library | Visualization | Creating publication-quality figures |

## 4.4 RAG (Retrieval-Augmented Generation) Stack

| Technology | What Is It | Where Used | Why Used |
|------------|------------|------------|----------|
| **Custom Knowledge Base** | JSON-structured medical facts | `knowledge_base.py` | Verified content, no hallucination, works offline, citable |
| **Keyword Matching** | Simple retrieval method | Knowledge retrieval | Transparent, explainable, sufficient for prototype |
| **XAI-to-Text Converter** | Heatmap analysis | `xai_to_text.py` | Novel contribution - bridges visualâ†’textual |

## 4.5 Data Processing

| Technology | What Is It | Where Used | Why Used |
|------------|------------|------------|----------|
| **PIL (Pillow)** | Image loading | `dataset.py` | Standard Python imaging library |
| **NumPy** | Numerical operations | Throughout | Array operations, statistics |
| **scikit-learn** | ML utilities | `metrics.py`, data splitting | Train/test split, evaluation metrics |
| **tqdm** | Progress bars | Training loops | Visual progress tracking |

## 4.6 Image Preprocessing Details

| Transform | Parameter | Why |
|-----------|-----------|-----|
| **Resize** | 224Ã—224 | Standard input size for ImageNet-pretrained models |
| **Normalize** | mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225] | ImageNet statistics - required for pretrained weights |
| **RandomHorizontalFlip** | p=0.5 | Data augmentation - lungs are roughly symmetric |
| **RandomRotation** | Â±15Â° | Data augmentation - handles slight orientation variations |
| **ColorJitter** | brightness/contrast=0.1 | Data augmentation - handles CT scanner variations |

### Why NOT Vertical Flip?
> Anatomically, lungs have top-bottom orientation that matters clinically. Flipping vertically would create unrealistic images.

---

# 5. MODULE-BY-MODULE BREAKDOWN

## 5.1 src/data/ - Data Handling

### dataset.py - Custom PyTorch Dataset
```python
class LungCancerDataset(Dataset):
    """
    WHAT: Custom dataset class for loading CT images
    WHY: Full control over data loading, can add metadata later
    HOW: 
        - Scans class folders
        - Loads images with PIL
        - Applies transforms
        - Returns (image_tensor, label) pairs
    """
```

**Key Methods:**
| Method | Purpose |
|--------|---------|
| `__init__` | Initialize paths, detect classes, load file list |
| `__len__` | Return total number of images |
| `__getitem__` | Load single image, apply transforms, return (image, label) |

### transforms.py - Image Preprocessing
```python
# Training: Augmentation + Normalization
get_train_transforms() â†’ Resize â†’ Flip â†’ Rotate â†’ ColorJitter â†’ ToTensor â†’ Normalize

# Validation/Test: Only Normalization (no augmentation)
get_val_transforms() â†’ Resize â†’ ToTensor â†’ Normalize
```

**Why Separate Transforms?**
> Training needs augmentation to increase data variety. Validation/Test needs deterministic, consistent preprocessing for reliable evaluation.

### dataloader.py - DataLoader Creation
```python
# Creates three dataloaders with proper splitting
train_loader, val_loader, test_loader = create_dataloaders(...)

# Split ratios: 70% train, 15% val, 15% test
# Stratified split ensures each set has proportional class representation
```

---

## 5.2 src/models/ - Neural Network

### classifier.py - Main Model
```python
class LungCancerClassifier(nn.Module):
    """
    Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                      ResNet-50 Backbone                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  conv1 â†’ bn1 â†’ relu â†’ maxpool                                â”‚
    â”‚      â†“                                                       â”‚
    â”‚  layer1 (64 â†’ 256 channels)    - 3 Bottleneck blocks        â”‚
    â”‚      â†“                                                       â”‚
    â”‚  layer2 (256 â†’ 512 channels)   - 4 Bottleneck blocks        â”‚
    â”‚      â†“                                                       â”‚
    â”‚  layer3 (512 â†’ 1024 channels)  - 6 Bottleneck blocks        â”‚
    â”‚      â†“                                                       â”‚
    â”‚  layer4 (1024 â†’ 2048 channels) - 3 Bottleneck blocks â† Grad-CAM target
    â”‚      â†“                                                       â”‚
    â”‚  avgpool (adaptive)                                          â”‚
    â”‚      â†“                                                       â”‚
    â”‚  Dropout(0.5)                                                â”‚
    â”‚      â†“                                                       â”‚
    â”‚  fc (2048 â†’ 5 classes)         - Modified for our task      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
```

**Key Implementation Details:**
```python
# Load pretrained weights
weights = models.ResNet50_Weights.IMAGENET1K_V2
self.backbone = models.resnet50(weights=weights)

# Modify final layer for our task
# Original: 2048 â†’ 1000 (ImageNet)
# Modified: 2048 â†’ 5 (Our classes)
self.backbone.fc = nn.Sequential(
    nn.Dropout(p=0.5),
    nn.Linear(2048, 5)
)
```

### model_factory.py - Model Creation
```python
def create_model(model_name, num_classes, pretrained, ...):
    """
    Factory pattern for creating models
    - Centralizes model creation
    - Easy to add new architectures
    - Consistent configuration
    """
```

---

## 5.3 src/xai/ - Explainable AI

### gradcam.py - Grad-CAM Implementation

**What is Grad-CAM?**
> Gradient-weighted Class Activation Mapping - uses gradients flowing into the final convolutional layer to produce a heatmap highlighting important regions.

**How It Works (Step by Step):**
```
Step 1: Forward Pass
        Input image â†’ Model â†’ Feature maps (A) captured at layer4
        
Step 2: Get Target Score
        Output logits â†’ Select score for target class (yá¶œ)
        
Step 3: Backward Pass
        Compute âˆ‚yá¶œ/âˆ‚A (gradients of target class w.r.t. feature maps)
        
Step 4: Global Average Pooling
        Î±â‚– = (1/Z) Î£áµ¢ Î£â±¼ (âˆ‚yá¶œ/âˆ‚Aâ‚–áµ¢â±¼)
        (Importance weight for each feature map k)
        
Step 5: Weighted Combination
        L = ReLU(Î£â‚– Î±â‚– Â· Aáµ)
        (Combine feature maps weighted by importance, keep only positive)
        
Step 6: Upscale
        Resize heatmap to input image size (224Ã—224)
```

**Implementation:**
```python
class GradCAM:
    def __init__(self, model, target_layer):
        # Register hooks to capture activations and gradients
        self.forward_handle = target_layer.register_forward_hook(forward_hook)
        self.backward_handle = target_layer.register_full_backward_hook(backward_hook)
    
    def generate(self, input_tensor, target_class=None):
        # 1. Forward pass
        output = self.model(input_tensor)
        
        # 2. Get target class score
        target_score = output[0, target_class]
        
        # 3. Backward pass
        target_score.backward()
        
        # 4. Compute weights (global average pooling of gradients)
        weights = self.gradients.mean(dim=[2, 3], keepdim=True)
        
        # 5. Weighted combination with ReLU
        cam = (weights * self.activations).sum(dim=1)
        cam = F.relu(cam)
        
        # 6. Normalize to [0, 1]
        cam = (cam - cam.min()) / (cam.max() - cam.min())
        
        return cam
```

**Why layer4?**
> Layer4 is the last convolutional layer in ResNet-50. It has:
> - Highest semantic meaning (understands "what" is in the image)
> - 7Ã—7 spatial resolution (enough for localization)
> - 2048 channels (rich feature representations)

### visualize.py - Visualization Utilities
```python
def create_heatmap_overlay(image, heatmap, alpha=0.4):
    """
    Overlays colored heatmap on original image
    
    Process:
    1. Normalize heatmap to [0, 255]
    2. Apply JET colormap (blueâ†’greenâ†’yellowâ†’red)
    3. Blend with original image (alpha blending)
    """
    
def visualize_gradcam(image, heatmap, predicted_class, confidence):
    """
    Creates 3-panel visualization:
    [Original Image] [Heatmap Only] [Overlay]
    """
```

---

## 5.4 src/rag/ - RAG Pipeline

### THIS IS THE NOVEL CONTRIBUTION

### knowledge_base.py - Medical Knowledge Store
```python
class MedicalKnowledgeBase:
    """
    WHAT: Structured repository of medical knowledge
    
    WHY Local (not LLM)?
    â”œâ”€â”€ âœ… Controlled content - all facts are verified
    â”œâ”€â”€ âœ… No hallucination - deterministic responses
    â”œâ”€â”€ âœ… Citable - every fact has a source
    â”œâ”€â”€ âœ… Works offline - no API dependency
    â””â”€â”€ âœ… Explainable - can show exactly what knowledge is used
    
    STRUCTURE:
    {
        "id": "adeno_001",
        "keywords": ["adenocarcinoma", "peripheral", "outer"],
        "content": "Adenocarcinoma typically presents in peripheral...",
        "source": "Travis WD et al., WHO Classification, 2021"
    }
    """
```

**Knowledge Retrieval Process:**
```
Query: "adenocarcinoma peripheral upper"
    â†“
Tokenize query into keywords
    â†“
Search keyword index
    â†“
Rank by keyword matches
    â†“
Return top-k entries with sources
```

### xai_to_text.py - XAI-to-Text Conversion (NOVEL!)
```python
class XAITextConverter:
    """
    THE BRIDGE FROM VISUAL TO TEXTUAL
    
    Input: Grad-CAM heatmap [H, W]
    Output: Textual description + Keywords for RAG
    
    Process:
    1. Analyze spatial distribution (divide into 3Ã—3 grid)
    2. Find high-attention regions
    3. Determine location (peripheral/central, upper/lower)
    4. Quantify intensity (very_high, high, moderate, low)
    5. Generate natural language description
    6. Extract keywords for knowledge base query
    """
```

**Spatial Analysis:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  top_left   â”‚ top_center  â”‚  top_right  â”‚
â”‚  (upper     â”‚  (upper     â”‚  (upper     â”‚
â”‚   left)     â”‚   central)  â”‚   right)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ middle_left â”‚middle_centerâ”‚ middle_rightâ”‚
â”‚  (middle    â”‚  (central)  â”‚  (middle    â”‚
â”‚   left)     â”‚             â”‚   right)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bottom_left â”‚bottom_centerâ”‚ bottom_rightâ”‚
â”‚  (lower     â”‚  (lower     â”‚  (lower     â”‚
â”‚   left)     â”‚   central)  â”‚   right)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For each region: Calculate mean attention value
Determine: Which regions have highest attention?
Map to: Medical terminology (peripheral = outer regions, central = middle)
```

### explanation_generator.py - Final Explanation
```python
class ExplanationGenerator:
    """
    Combines everything into coherent explanation
    
    Components:
    1. XAITextConverter - "Model focused on peripheral upper region"
    2. MedicalKnowledgeBase - "Adenocarcinoma typically presents peripherally"
    
    Output Format:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PREDICTION: Adenocarcinoma (92.3%)                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ VISUAL EVIDENCE:                                            â”‚
    â”‚ The model focused on the peripheral upper region with       â”‚
    â”‚ high attention intensity, concentrated in specific areas.   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ MEDICAL CONTEXT:                                            â”‚
    â”‚ Adenocarcinoma typically presents in the peripheral regions â”‚
    â”‚ of the lung. Ground-glass opacity is frequently associated  â”‚
    â”‚ with this cancer type.                                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Sources: Travis WD et al., WHO Classification 2021          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
```

---

## 5.5 src/utils/ - Utilities

### config.py - Centralized Configuration
```python
@dataclass
class Config:
    # Paths
    base_dir = "d:\\Major Project"
    dataset_dir = "archive (1)/Lung Cancer Dataset"
    
    # Classes
    class_names = ["adenocarcinoma", "Benign cases", 
                   "large cell carcinoma", "Normal cases",
                   "squamous cell carcinoma"]
    num_classes = 5
    
    # Training
    image_size = (224, 224)
    batch_size = 32
    num_epochs = 10
    learning_rate = 1e-4
    
    # Model
    model_name = "resnet50"
    pretrained = True
    dropout_rate = 0.5
```

### metrics.py - Evaluation Metrics
```python
# Metrics used:
accuracy_score()   # Overall correctness
precision_score()  # How many predicted positives are true
recall_score()     # How many actual positives were found
f1_score()         # Harmonic mean of precision & recall
confusion_matrix() # Full breakdown of predictions
```

**Why Recall is Important in Medical Imaging:**
> Missing a cancer case (False Negative) is more dangerous than a false alarm (False Positive). We want HIGH RECALL to catch all potential cancers.

---

# 6. KEY ALGORITHMS & TECHNIQUES

## 6.1 Transfer Learning

**Concept:**
```
ImageNet (1.2M images, 1000 classes)
         â†“ Pre-training
    ResNet-50 learns general features
    (edges, textures, shapes, objects)
         â†“ Transfer
    Load these weights
         â†“ Fine-tune
    Train on our lung cancer dataset
    (Only modify final layers significantly)
```

**Why It Works:**
> Low-level features (edges, textures) learned on natural images transfer well to medical images. High-level features get adapted during fine-tuning.

## 6.2 Residual Connections (ResNet)

**Problem:** Very deep networks suffer from vanishing gradients
**Solution:** Skip connections that add input to output

```
Traditional:      Residual:
x                 x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â†“                 â†“                      â”‚
Conv              Conv                   â”‚
â†“                 â†“                      â”‚
Conv              Conv                   â”‚
â†“                 â†“                      â”‚
y                 y = F(x) + x â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â””â”€â”€ Residual connection
```

**Mathematical Form:**
```
H(x) = F(x) + x

Where:
- x is the input
- F(x) is the learned residual
- H(x) is the output
```

## 6.3 Cross-Entropy Loss

**Used for:** Multi-class classification

**Formula:**
$$L = -\sum_{c=1}^{C} y_c \log(p_c)$$

Where:
- $C$ = number of classes (5)
- $y_c$ = 1 if correct class, 0 otherwise
- $p_c$ = predicted probability for class c

## 6.4 AdamW Optimizer

**Why AdamW?**
- Adaptive learning rate per parameter
- Weight decay for regularization
- Works well with pretrained models

**Key Parameters:**
```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=1e-4,           # Learning rate
    weight_decay=1e-4  # L2 regularization
)
```

---

# 7. DATASET DETAILS

## 7.1 Dataset Source
**Kaggle Lung Cancer CT Scan Dataset**

## 7.2 Dataset Structure
```
archive (1)/Lung Cancer Dataset/
â”œâ”€â”€ adenocarcinoma/          # ~1500 images
â”œâ”€â”€ Benign cases/            # ~1500 images  
â”œâ”€â”€ large cell carcinoma/    # ~1500 images
â”œâ”€â”€ Normal cases/            # ~1500 images
â””â”€â”€ squamous cell carcinoma/ # ~1500 images
```

## 7.3 Image Characteristics
| Property | Value |
|----------|-------|
| Format | PNG/JPG |
| Original Size | Variable |
| Processed Size | 224Ã—224 |
| Color | RGB (converted if grayscale) |
| Source | CT scan slices |

## 7.4 Data Split
| Split | Percentage | Purpose |
|-------|------------|---------|
| Training | 70% | Learning model weights |
| Validation | 15% | Hyperparameter tuning, early stopping |
| Test | 15% | Final unbiased evaluation |

**Stratified Split:** Ensures each split has proportional class representation

---

# 8. TRAINING PROCESS

## 8.1 Training Pipeline
```
for epoch in range(num_epochs):
    # Training Phase
    model.train()
    for batch in train_loader:
        1. Forward pass: predictions = model(images)
        2. Compute loss: loss = criterion(predictions, labels)
        3. Backward pass: loss.backward()
        4. Update weights: optimizer.step()
    
    # Validation Phase
    model.eval()
    with torch.no_grad():
        Evaluate on validation set
        Track validation accuracy
    
    # Save best model if validation improves
    if val_accuracy > best_accuracy:
        save_checkpoint(model, "best_model.pth")
```

## 8.2 Training Configuration
| Hyperparameter | Value | Reason |
|----------------|-------|--------|
| Epochs | 10 | Sufficient for convergence with transfer learning |
| Batch Size | 32 | Balance between memory and gradient stability |
| Learning Rate | 1e-4 | Conservative for fine-tuning pretrained models |
| Optimizer | AdamW | Adaptive + weight decay |
| Loss | CrossEntropyLoss | Standard for multi-class |

## 8.3 Checkpoints Saved
| File | Description |
|------|-------------|
| `best_model.pth` | Model with highest validation accuracy |
| `final_model.pth` | Model after all epochs complete |

---

# 9. EVALUATION METRICS

## 9.1 Primary Metrics

### Accuracy
$$\text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}}$$

### Precision (Per Class)
$$\text{Precision} = \frac{TP}{TP + FP}$$
> "Of all predicted as X, how many are actually X?"

### Recall (Per Class)
$$\text{Recall} = \frac{TP}{TP + FN}$$
> "Of all actual X, how many did we find?"

### F1-Score
$$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$
> Harmonic mean of precision and recall

## 9.2 Confusion Matrix
```
                    Predicted
                A    SCC   LCC   B    N
         A     [TP   ...   ...  ...  ...]
Actual  SCC    [...   TP   ...  ...  ...]
        LCC    [...  ...   TP   ...  ...]
         B     [...  ...  ...   TP  ...]
         N     [...  ...  ...  ...   TP]

Where: A=Adenocarcinoma, SCC=Squamous, LCC=Large Cell, B=Benign, N=Normal
```

---

# 10. POTENTIAL QUESTIONS & ANSWERS

## Architecture Questions

### Q1: Why ResNet-50 and not other architectures?
**A:** ResNet-50 offers the best balance for our use case:
1. **Proven medical imaging performance** - widely used in literature
2. **Residual connections** - solve vanishing gradient in deep networks
3. **2048 features** - rich representations for cancer patterns
4. **Grad-CAM compatible** - layer4 produces clear, interpretable heatmaps
5. **Well-documented** - extensive research papers and tutorials
6. **Transfer learning ready** - ImageNet pretraining provides good initialization

### Q2: Why 224Ã—224 image size?
**A:** This is the standard input size for ImageNet-pretrained models:
1. The pretrained convolutional filters expect this resolution
2. Changing size would require architecture modifications
3. It provides sufficient detail for classification while being computationally efficient

### Q3: What are residual connections and why do they matter?
**A:** Residual connections add the input directly to the output: H(x) = F(x) + x
- **Solves vanishing gradient**: Gradients can flow directly through skip connections
- **Enables depth**: Without them, networks >20 layers become hard to train
- **Identity mapping**: If F(x)=0, the block simply passes input through

### Q4: Why transfer learning for medical imaging?
**A:** Medical datasets are typically small (thousands vs millions). Transfer learning helps because:
1. **Feature reuse**: Low-level features (edges, textures) transfer across domains
2. **Better initialization**: Pretrained weights start closer to optimal
3. **Faster convergence**: Fewer epochs needed
4. **Reduced overfitting**: More generalizable features

## Explainability Questions

### Q5: What is Grad-CAM and how does it work?
**A:** Grad-CAM (Gradient-weighted Class Activation Mapping) creates visual explanations:
1. **Forward pass**: Get feature maps from target layer (layer4)
2. **Backward pass**: Compute gradients of target class score w.r.t. feature maps
3. **Weight computation**: Global average pool gradients to get importance weights
4. **Combine**: Weighted sum of feature maps
5. **ReLU**: Keep only positive activations (regions that help the prediction)
6. **Resize**: Upscale to original image size

### Q6: Why Grad-CAM instead of other XAI methods?
**A:** 
| Method | Pros | Cons | Why Not |
|--------|------|------|---------|
| Grad-CAM | No modification needed, class-specific | Coarse resolution | âœ… Selected |
| LIME | Model-agnostic | Slow, many parameters | Too complex |
| SHAP | Theoretically sound | Computationally expensive | Too slow |
| Attention | Built-in for transformers | Requires architecture change | Not applicable to ResNet |

### Q7: What is the novel contribution - XAI-to-Text?
**A:** The XAI-to-Text converter bridges visual explanations with textual knowledge:
1. **Analyzes heatmap spatially** - divides into 3Ã—3 grid
2. **Identifies attention regions** - peripheral, central, upper, lower
3. **Quantifies intensity** - very high, high, moderate, low
4. **Generates keywords** - "adenocarcinoma peripheral upper"
5. **Queries knowledge base** - retrieves relevant medical facts

This is novel because it connects WHERE the model looks with WHY that matters medically.

## RAG Questions

### Q8: What is RAG and why use it?
**A:** RAG = Retrieval-Augmented Generation
- **Concept**: Instead of generating explanations from scratch, retrieve relevant facts from a curated knowledge base
- **Why**: Ensures accuracy, provides citations, no hallucination
- **Implementation**: Simple keyword matching (can upgrade to semantic search later)

### Q9: Why not use an LLM like GPT for explanations?
**A:**
| LLM Approach | Our Approach |
|--------------|--------------|
| Can hallucinate | Deterministic, verified facts |
| Black-box generation | Transparent retrieval |
| Needs API, costs money | Works offline, free |
| Hard to cite sources | Every fact has a citation |
| May be wrong | Content is curated by us |

For medical applications, reliability and citation are critical.

### Q10: How is the knowledge base structured?
**A:** JSON structure with:
```json
{
  "id": "adeno_001",
  "keywords": ["adenocarcinoma", "peripheral", "outer"],
  "content": "Medical fact about adenocarcinoma...",
  "source": "Travis WD et al., WHO Classification, 2021"
}
```
- **Keywords**: Trigger terms for retrieval
- **Content**: Verified medical information
- **Source**: Academic citation

## Training Questions

### Q11: Why 70-15-15 split?
**A:** Standard split for ML projects:
- **70% Training**: Main learning data
- **15% Validation**: Tune hyperparameters, monitor for overfitting
- **15% Test**: Final unbiased evaluation (never seen during training)

Stratified split ensures each set has proportional class distribution.

### Q12: What is data augmentation and why use it?
**A:** Data augmentation artificially increases training data variety:
- **Horizontal flip**: Lungs are roughly symmetric
- **Rotation (Â±15Â°)**: Handles slight orientation variations
- **Color jitter**: Handles CT scanner variations

**Why**: Prevents overfitting, improves generalization, especially important with limited medical data.

### Q13: Why use dropout?
**A:** Dropout randomly sets neurons to 0 during training:
- **Rate 0.5**: 50% of neurons dropped each iteration
- **Effect**: Prevents co-adaptation of neurons
- **Result**: More robust features, better generalization

### Q14: Why AdamW optimizer?
**A:** AdamW = Adam with decoupled weight decay
- **Adaptive learning rate**: Different rate per parameter
- **Momentum**: Smooths gradient updates
- **Weight decay**: L2 regularization for better generalization
- **Works well with pretrained models**

## Metrics Questions

### Q15: Why is recall important in medical imaging?
**A:** In cancer detection:
- **False Negative** (missing cancer) = Dangerous, patient doesn't get treatment
- **False Positive** (false alarm) = Less dangerous, just additional tests

We want HIGH RECALL to minimize false negatives.

### Q16: What does the confusion matrix tell us?
**A:** Shows detailed breakdown:
- Diagonal = correct predictions
- Off-diagonal = where model confuses classes
- Helps identify: Which cancers are confused with each other?

## Technical Questions

### Q17: What are forward and backward hooks?
**A:** PyTorch hooks capture intermediate values:
- **Forward hook**: Captures output of a layer during forward pass
- **Backward hook**: Captures gradients during backward pass
- **Usage in Grad-CAM**: Capture feature maps and gradients for heatmap computation

### Q18: What is the model's output format?
**A:** 
- **Raw output**: Logits (unnormalized scores) for each class
- **After softmax**: Probabilities summing to 1
- **Prediction**: argmax of probabilities

### Q19: How do you handle class imbalance?
**A:** 
- **Stratified split**: Ensures proportional class distribution
- **Weighted loss** (optional): Give more weight to minority classes
- **Data augmentation**: Increases effective training data
- **Metrics**: Use macro-averaged F1 instead of accuracy

### Q20: What happens during inference (prediction)?
**A:**
```
1. Load image â†’ Preprocess (resize, normalize)
2. Forward pass â†’ Get logits
3. Softmax â†’ Get probabilities
4. Grad-CAM â†’ Get heatmap
5. XAI-to-Text â†’ Get description
6. RAG â†’ Get medical context
7. Combine â†’ Final explanation
```

---

# 11. QUICK REFERENCE CARDS

## 11.1 Pipeline Summary Card
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LUNGXAI PIPELINE SUMMARY                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ INPUT:  CT Scan Image (any size, PNG/JPG)           â”‚
â”‚         â†“                                           â”‚
â”‚ PREPROCESS: Resize 224Ã—224, Normalize               â”‚
â”‚         â†“                                           â”‚
â”‚ MODEL: ResNet-50 (pretrained, fine-tuned)           â”‚
â”‚         â†“                                           â”‚
â”‚ OUTPUT: 5-class probabilities                       â”‚
â”‚         â†“                                           â”‚
â”‚ XAI: Grad-CAM heatmap (layer4)                      â”‚
â”‚         â†“                                           â”‚
â”‚ CONVERT: XAI-to-Text (spatial analysis)             â”‚
â”‚         â†“                                           â”‚
â”‚ RAG: Knowledge retrieval (keyword matching)         â”‚
â”‚         â†“                                           â”‚
â”‚ FINAL: Visual + Textual explanation                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 11.2 Technology Quick Reference
| Component | Technology | File |
|-----------|------------|------|
| Model | ResNet-50 + PyTorch | classifier.py |
| XAI | Grad-CAM | gradcam.py |
| Knowledge | JSON Knowledge Base | knowledge_base.py |
| Bridge | XAI-to-Text Converter | xai_to_text.py |
| Visualization | OpenCV + Matplotlib | visualize.py |

## 11.3 Key Numbers
| Metric | Value |
|--------|-------|
| Input Size | 224 Ã— 224 Ã— 3 |
| Model Parameters | ~25.6M (ResNet-50) |
| Feature Size | 2048 (before FC) |
| Output Classes | 5 |
| Grad-CAM Layer | layer4 (7Ã—7Ã—2048) |
| Dropout Rate | 0.5 |
| Learning Rate | 1e-4 |

## 11.4 Class Information Card
| Class | Typical CT Features | Typical Location |
|-------|---------------------|------------------|
| Adenocarcinoma | Ground-glass opacity, spiculated margins | Peripheral |
| Squamous Cell | Central mass, cavitation | Central/Hilar |
| Large Cell | Large peripheral mass | Peripheral |
| Benign | Well-defined margins, calcification | Variable |
| Normal | Clear lung fields | N/A |

## 11.5 Formula Quick Reference
```
Grad-CAM:    L = ReLU(Î£â‚– Î±â‚– Â· Aáµ)   where Î±â‚– = GAP(âˆ‚yá¶œ/âˆ‚Aáµ)

Loss:        L = -Î£ yc log(pc)

Accuracy:    TP + TN / Total

Precision:   TP / (TP + FP)

Recall:      TP / (TP + FN)

F1:          2 Ã— (P Ã— R) / (P + R)
```

---

# FINAL TIPS FOR REVIEW

1. **Start with the big picture** - Explain the problem, then the solution
2. **Know the "why"** - Be prepared to justify every technology choice
3. **Understand the flow** - Be able to trace data through the entire pipeline
4. **Remember key numbers** - 224Ã—224, 5 classes, 2048 features, layer4
5. **Emphasize novelty** - XAI-to-Text conversion is your unique contribution
6. **Clinical relevance** - Always connect back to why this helps doctors

**Good luck with your review! ğŸ“**

---

*Document generated for Major Project Review - January 2026*
