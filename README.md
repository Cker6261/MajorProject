# ü´Å LungXAI - Explainable AI for Lung Cancer Classification

[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

> **Bridging the gap between AI predictions and clinical understanding through visual and textual explanations**

An advanced deep learning system that classifies lung CT scan images into cancer types while providing explainable AI insights through visual heatmaps and medical knowledge retrieval.

## üéØ Project Overview

This project implements an **explainable AI system** for classifying lung CT images into five categories:
- **Adenocarcinoma** - Most common type of lung cancer
- **Squamous Cell Carcinoma** - Often found in central lung areas
- **Large Cell Carcinoma** - Fast-growing cancer type
- **Benign Cases** - Non-cancerous tissue
- **Normal Cases** - Healthy lung tissue

### What Makes This Project Unique?

Traditional medical AI systems provide predictions but lack interpretability. This project bridges that gap by:

1. **Multi-Model Classification**: Comparing 4 different deep learning architectures:
   - **ResNet-50**: Classic residual network (96.97% accuracy)
   - **MobileNetV2**: Lightweight model for deployment (97.40% accuracy)
   - **Vision Transformer (ViT)**: Attention-based architecture (93.51% accuracy)
   - **Swin Transformer**: Hierarchical transformer with shifted windows (97.84% accuracy - **Best!**)

2. **Visual Explanation**: Generating Grad-CAM heatmaps to show WHERE the model is looking
3. **Textual Explanation**: Using RAG to explain WHY those regions are significant
4. **Model Comparison**: Built-in tools to compare all models and select the best one

### Key Features

- **Caching Support**: Models are cached after training - no retraining needed!
- **Multi-Model Training**: Train all models with a single command
- **Automatic Comparison**: Generate comparison charts and reports
- **Memory Efficient**: Sequential training to prevent GPU memory issues
- **D: Drive Storage**: All data stored on D: drive to prevent C: drive issues

---

## üìÅ Project Structure

```
Major Project/
‚îÇ
‚îú‚îÄ‚îÄ main.py                    # Main entry point
‚îú‚îÄ‚îÄ train_all_models.py        # Train all models with caching
‚îú‚îÄ‚îÄ compare_models.py          # Compare all trained models
‚îú‚îÄ‚îÄ demo_multi_model.py        # Demo with model selection
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ README.md                  # This file
‚îÇ
‚îú‚îÄ‚îÄ src/                       # Source code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/                  # Data handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py         # Custom PyTorch Dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transforms.py      # Image augmentation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataloader.py      # DataLoader utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Neural network models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.py      # ResNet-50 classifier
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_factory.py   # Factory for all models
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ xai/                   # Explainable AI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gradcam.py         # Grad-CAM implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualize.py       # Visualization utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ rag/                   # RAG Pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ knowledge_base.py        # Medical knowledge store
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pubmed_retriever.py      # PubMed API integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xai_to_text.py           # XAI ‚Üí Text conversion
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ explanation_generator.py # Full explanation generation
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py          # Centralized configuration
‚îÇ       ‚îú‚îÄ‚îÄ helpers.py         # Helper functions
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py         # Evaluation metrics
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/               # Saved model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ best_model_resnet50.pth
‚îÇ   ‚îú‚îÄ‚îÄ best_model_mobilenetv2.pth
‚îÇ   ‚îú‚îÄ‚îÄ best_model_vit_b_16.pth
‚îÇ   ‚îî‚îÄ‚îÄ best_model_swin_t.pth
‚îÇ
‚îú‚îÄ‚îÄ results/                   # Output results
‚îÇ   ‚îú‚îÄ‚îÄ comparison/            # Model comparison charts
‚îÇ   ‚îú‚îÄ‚îÄ resnet50/              # ResNet-50 specific results
‚îÇ   ‚îú‚îÄ‚îÄ mobilenetv2/           # MobileNetV2 specific results
‚îÇ   ‚îú‚îÄ‚îÄ vit_b_16/              # ViT specific results
‚îÇ   ‚îî‚îÄ‚îÄ swin_t/                # Swin Transformer specific results
‚îÇ
‚îî‚îÄ‚îÄ archive (1)/               # Dataset
    ‚îî‚îÄ‚îÄ Lung Cancer Dataset/
        ‚îú‚îÄ‚îÄ adenocarcinoma/
        ‚îú‚îÄ‚îÄ Benign cases/
        ‚îú‚îÄ‚îÄ large cell carcinoma/
        ‚îú‚îÄ‚îÄ Normal cases/
        ‚îî‚îÄ‚îÄ squamous cell carcinoma/
```

### Why This Structure?

| Directory | Purpose | Academic Justification |
|-----------|---------|----------------------|
| `src/data/` | Data loading & preprocessing | Separates data concerns from model logic |
| `src/models/` | Neural network architectures | Allows easy model comparison |
| `src/xai/` | Explainability methods | Isolates XAI implementation for clarity |
| `src/rag/` | Knowledge retrieval | Novel contribution - bridges XAI to explanations |
| `src/utils/` | Common utilities | Reduces code duplication |
| `notebooks/` | Experiments | Interactive development and visualization |

## üöÄ Quick Start

```bash
# Clone the repository
git clone https://github.com/yourusername/LungXAI.git
cd LungXAI

# Create and activate virtual environment
python -m venv .venv
# Windows
.\.venv\Scripts\activate
# Linux/Mac
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Download dataset (see Dataset section below)
# Then train all models
python train_all_models.py

# Run demo with all models
python demo_multi_model.py --compare
```

## üìä Key Results

| Model | Test Accuracy | Parameters | Best For |
|-------|---------------|------------|----------|
| **Swin-T** üèÜ | **97.84%** | ~28M | **Best overall performance** |
| MobileNetV2 ‚ö° | 97.40% | ~3.5M | Edge deployment |
| ResNet-50 üîç | 96.97% | ~25.6M | Excellent explainability |
| ViT-B/16 üß† | 93.51% | ~86M | Research applications |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         INPUT: CT SCAN IMAGE                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PREPROCESSING (224x224, Normalize)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DEEP LEARNING MODEL (Pretrained, Fine-tuned)            ‚îÇ
‚îÇ                                                                       ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ    ‚îÇ   Features   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  Prediction  ‚îÇ        ‚îÇ
‚îÇ    ‚îÇ   (layer4)   ‚îÇ                          ‚îÇ   (4 class)  ‚îÇ        ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ           ‚îÇ                                         ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                         ‚îÇ
            ‚ñº                                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      GRAD-CAM        ‚îÇ                    ‚îÇ  CLASS PREDICTION    ‚îÇ
‚îÇ    (Visual XAI)      ‚îÇ                    ‚îÇ  + Confidence Score  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                         ‚îÇ
            ‚ñº                                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ
‚îÇ  XAI ‚Üí TEXT BRIDGE   ‚îÇ                              ‚îÇ
‚îÇ  "peripheral opacity"‚îÇ                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ
            ‚îÇ                                         ‚îÇ
            ‚ñº                                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ
‚îÇ  KNOWLEDGE RETRIEVAL ‚îÇ                              ‚îÇ
‚îÇ   (Medical Facts)    ‚îÇ                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ
            ‚îÇ                                         ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FINAL OUTPUT                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Prediction  ‚îÇ  ‚îÇ  Grad-CAM   ‚îÇ  ‚îÇ      RAG Explanation        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇAdenocarcinoma‚îÇ  ‚îÇ  Heatmap    ‚îÇ  ‚îÇ "Ground-glass opacity..."  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (92%)     ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ                             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ÔøΩ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-compatible GPU (recommended)
- At least 8GB RAM
- 10GB free disk space

### Step 1: Clone Repository
```bash
git clone https://github.com/yourusername/LungXAI.git
cd LungXAI
```

### Step 2: Environment Setup
```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows PowerShell
.\.venv\Scripts\Activate.ps1
# Windows CMD
.venv\Scripts\activate.bat
# Linux/Mac
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Step 3: Verify Installation
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

---

## üìä Dataset

**Source**: [CT Scan Images of Lung Cancer Patients](https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images) (Kaggle)

### Download Instructions
1. Visit the Kaggle dataset link above
2. Download and extract the dataset
3. Place in the project directory as:
```
LungXAI/
‚îî‚îÄ‚îÄ archive (1)/
    ‚îî‚îÄ‚îÄ Lung Cancer Dataset/
        ‚îú‚îÄ‚îÄ adenocarcinoma/
        ‚îú‚îÄ‚îÄ Benign cases/
        ‚îú‚îÄ‚îÄ large cell carcinoma/
        ‚îú‚îÄ‚îÄ Normal cases/
        ‚îî‚îÄ‚îÄ squamous cell carcinoma/
```

### Dataset Statistics
| Class | Images | Description |
|-------|--------|-------------|
| Adenocarcinoma | ~150 | Most common lung cancer type |
| Squamous Cell | ~150 | Central lung areas |
| Large Cell | ~150 | Fast-growing type |
| Benign | ~150 | Non-cancerous tissue |
| Normal | ~150 | Healthy lung tissue |

---

## üéÆ Usage

### üöÄ Training All Models (Recommended)

```bash
# Train all models with caching (skip already trained models)
python train_all_models.py

# Force retrain all models
python train_all_models.py --force-retrain

# Train specific models only
python train_all_models.py --models resnet50 mobilenetv2
```

### üîç Demo & Inference

```bash
# Demo with default model (ResNet-50)
python demo_multi_model.py

# Demo with specific model
python demo_multi_model.py --model swin_t        # Best accuracy
python demo_multi_model.py --model mobilenetv2  # Fastest
python demo_multi_model.py --model vit_b_16      # Research

# Compare all models on same image
python demo_multi_model.py --compare

# List available models and training status
python demo_multi_model.py --list

# Visual demo with custom image
python demo.py path/to/your/ct_scan.png
```

### üìä Model Comparison & Evaluation

```bash
# Generate comprehensive model comparison
python compare_models.py

# Evaluate specific model
python evaluate_model.py --model swin_t

# Test RAG explanation system
python test_rag_pipeline.py
```

### üîß Individual Operations

```bash
# Train single model (legacy method)
python main.py --mode train --epochs 30

# Evaluate on test set
python main.py --mode evaluate --checkpoint checkpoints/best_model_resnet50.pth

# Predict single image
python main.py --mode predict --image path/to/ct_scan.png

# Run interactive demo
python main.py --mode demo
```

---

## üìä Model Performance

*Results after training on the Lung Cancer CT Scan Dataset (5 classes)*

| Model | Accuracy | Precision | Recall | F1-Score | Training Time | Parameters |
|-------|----------|-----------|--------|----------|---------------|------------|
| **Swin-T** ü•á | **97.84%** | **97.86%** | **97.84%** | **97.84%** | ~28 min | 28M |
| MobileNetV2 ü•à | **97.40%** | **97.50%** | **97.40%** | **97.40%** | ~17 min | 3.5M |
| ResNet-50 ü•â | 96.97% | 96.99% | 96.97% | 96.95% | ~7 min | 25.6M |
| ViT-B/16 | 93.51% | 93.74% | 93.51% | 93.48% | ~80 min | 86M |

### üéØ Model Selection Guide

- **üèÜ Best Overall**: **Swin Transformer (Tiny)** - Highest accuracy with reasonable training time
- **‚ö° Deployment**: **MobileNetV2** - Excellent accuracy-to-efficiency ratio
- **üîç Explainability**: **ResNet-50** - Superior Grad-CAM visualizations
- **üß† Research**: **ViT-B/16** - Cutting-edge transformer architecture

---

## ü§ù Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Quick Start for Contributors
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests if applicable
5. Commit changes (`git commit -m 'Add amazing feature'`)
6. Push to branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

---

## üêõ Known Issues & Limitations

1. **Dataset Size**: Limited medical imaging data may affect generalization
2. **Grad-CAM**: Shows correlation, not causation; may highlight spurious features
3. **RAG Simplicity**: Current implementation uses keyword matching; semantic search would be better
4. **Clinical Validation**: Not validated by medical professionals - **NOT FOR CLINICAL USE**
5. **GPU Memory**: Large models (ViT) require significant GPU memory

---

## üîÆ Roadmap & Future Enhancements

### Planned Features
- [ ] **Semantic RAG**: Upgrade to sentence transformers for better knowledge retrieval
- [ ] **Multiple XAI Methods**: Add LIME, SHAP for comprehensive explanations
- [ ] **Web Interface**: User-friendly web application for easier access
- [ ] **Larger Dataset**: Integration with additional medical image datasets
- [ ] **Clinical Validation**: Collaboration with medical professionals

### Technical Improvements
- [ ] **Model Ensemble**: Combine predictions from multiple models
- [ ] **Real-time Inference**: Optimize for faster prediction times
- [ ] **Cloud Deployment**: Docker containerization and cloud deployment guides
- [ ] **Mobile App**: Mobile application for edge deployment

---

## ‚ö†Ô∏è Important Disclaimers

> **‚ö†Ô∏è NOT FOR CLINICAL USE**: This project is for research and educational purposes only. It has not been validated by medical professionals and should not be used for actual medical diagnosis or treatment decisions.

> **üìö Academic Use**: This project is developed for academic research and learning purposes. Always consult qualified medical professionals for health-related decisions.

---

## üìö Documentation

- [**Complete User Guide**](docs/PROJECT_REVIEW_GUIDE.md) - Comprehensive project documentation
- [**Command Reference**](COMMANDS.md) - All available commands and usage examples
- [**Research Paper**](docs/LungXAI_Research_Paper.md) - Academic paper with technical details
- [**Pipeline Architecture**](docs/PSEUDOCODE.md) - Technical implementation details
- [**Model Comparison**](docs/BASELINE_VS_FINETUNED_COMPARISON.md) - Detailed model analysis

---

## üìú Citation

If you use this work in your research, please cite:

```bibtex
@misc{lungxai2024,
  title={Explainable AI for Multi-Class Lung Cancer Classification Using Deep Learning and RAG-Based Knowledge Retrieval},
  author={Major Project Team},
  year={2024},
  howpublished={\url{https://github.com/yourusername/LungXAI}}
}
```

---

## üìö References

1. **Selvaraju, R. R., et al.** (2017). "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization." *ICCV 2017*.
2. **He, K., et al.** (2016). "Deep Residual Learning for Image Recognition." *CVPR 2016*.
3. **Lewis, P., et al.** (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." *NeurIPS 2020*.
4. **Liu, Z., et al.** (2021). "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows." *ICCV 2021*.
5. **Dosovitskiy, A., et al.** (2020). "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." *ICLR 2021*.

---

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/LungXAI/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/LungXAI/discussions)
- **Email**: your.email@university.edu

---

## üë• Authors & Contributors

**Major Project Team** - *Final Year B.Tech Computer Science*
- Lead Developer: [Your Name]
- Contributors: [Team Member 1], [Team Member 2]

See the full list of [contributors](https://github.com/yourusername/LungXAI/contributors) who participated in this project.

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

**Academic Use Encouraged**: This project is developed for educational and research purposes. We encourage its use in academic settings with proper attribution.

---

## üåü Star History

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/LungXAI&type=Date)](https://star-history.com/#yourusername/LungXAI&Date)

---

## üôè Acknowledgments

- **Kaggle Community** for providing the lung cancer CT scan dataset
- **PyTorch Team** for the excellent deep learning framework
- **Hugging Face** for transformer models and tools
- **Scientific Community** for open-source medical AI research
- **University Faculty** for guidance and support

---

<div align="center">

**Made with ‚ù§Ô∏è for the advancement of medical AI research**

[‚≠ê Star this repo](https://github.com/yourusername/LungXAI/stargazers) | [üêõ Report Bug](https://github.com/yourusername/LungXAI/issues) | [‚ú® Request Feature](https://github.com/yourusername/LungXAI/issues)

</div> 
