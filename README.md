# Explainable AI for Multi-Class Lung Cancer Classification

## Using Deep Learning and RAG-Based Knowledge Retrieval

---

## ğŸ¯ Project Overview

This project implements an **explainable AI system** for classifying lung CT images into five categories:
- **Adenocarcinoma** - Most common type of lung cancer
- **Squamous Cell Carcinoma** - Often found in central lung areas
- **Large Cell Carcinoma** - Fast-growing cancer type
- **Benign Cases** - Non-cancerous tissue
- **Normal Cases** - Healthy lung tissue

### What Makes This Project Unique?

Traditional medical AI systems provide predictions but lack interpretability. This project bridges that gap by:

1. **Multi-Model Classification**: Comparing 4 different deep learning architectures:
   - **ResNet-50**: Classic residual network (96.97% accuracy)
   - **MobileNetV2**: Lightweight model for deployment (97.40% accuracy)
   - **Vision Transformer (ViT)**: Attention-based architecture (93.51% accuracy)
   - **Swin Transformer**: Hierarchical transformer with shifted windows (97.84% accuracy - **Best!**)

2. **Visual Explanation**: Generating Grad-CAM heatmaps to show WHERE the model is looking
3. **Textual Explanation**: Using RAG to explain WHY those regions are significant
4. **Model Comparison**: Built-in tools to compare all models and select the best one

### Key Features

- **Caching Support**: Models are cached after training - no retraining needed!
- **Multi-Model Training**: Train all models with a single command
- **Automatic Comparison**: Generate comparison charts and reports
- **Memory Efficient**: Sequential training to prevent GPU memory issues
- **D: Drive Storage**: All data stored on D: drive to prevent C: drive issues

---

## ğŸ“ Project Structure

```
Major Project/
â”‚
â”œâ”€â”€ main.py                    # Main entry point
â”œâ”€â”€ train_all_models.py        # Train all models with caching
â”œâ”€â”€ compare_models.py          # Compare all trained models
â”œâ”€â”€ demo_multi_model.py        # Demo with model selection
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This file
â”‚
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                  # Data handling
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py         # Custom PyTorch Dataset
â”‚   â”‚   â”œâ”€â”€ transforms.py      # Image augmentation
â”‚   â”‚   â””â”€â”€ dataloader.py      # DataLoader utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                # Neural network models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ classifier.py      # ResNet-50 classifier
â”‚   â”‚   â””â”€â”€ model_factory.py   # Factory for all models
â”‚   â”‚
â”‚   â”œâ”€â”€ xai/                   # Explainable AI
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gradcam.py         # Grad-CAM implementation
â”‚   â”‚   â””â”€â”€ visualize.py       # Visualization utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/                   # RAG Pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ knowledge_base.py        # Medical knowledge store
â”‚   â”‚   â”œâ”€â”€ pubmed_retriever.py      # PubMed API integration
â”‚   â”‚   â”œâ”€â”€ xai_to_text.py           # XAI â†’ Text conversion
â”‚   â”‚   â””â”€â”€ explanation_generator.py # Full explanation generation
â”‚   â”‚
â”‚   â””â”€â”€ utils/                 # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py          # Centralized configuration
â”‚       â”œâ”€â”€ helpers.py         # Helper functions
â”‚       â””â”€â”€ metrics.py         # Evaluation metrics
â”‚
â”œâ”€â”€ checkpoints/               # Saved model checkpoints
â”‚   â”œâ”€â”€ best_model_resnet50.pth
â”‚   â”œâ”€â”€ best_model_mobilenetv2.pth
â”‚   â”œâ”€â”€ best_model_vit_b_16.pth
â”‚   â””â”€â”€ best_model_swin_t.pth
â”‚
â”œâ”€â”€ results/                   # Output results
â”‚   â”œâ”€â”€ comparison/            # Model comparison charts
â”‚   â”œâ”€â”€ resnet50/              # ResNet-50 specific results
â”‚   â”œâ”€â”€ mobilenetv2/           # MobileNetV2 specific results
â”‚   â”œâ”€â”€ vit_b_16/              # ViT specific results
â”‚   â””â”€â”€ swin_t/                # Swin Transformer specific results
â”‚
â””â”€â”€ archive (1)/               # Dataset
    â””â”€â”€ Lung Cancer Dataset/
        â”œâ”€â”€ adenocarcinoma/
        â”œâ”€â”€ Benign cases/
        â”œâ”€â”€ large cell carcinoma/
        â”œâ”€â”€ Normal cases/
        â””â”€â”€ squamous cell carcinoma/
```

### Why This Structure?

| Directory | Purpose | Academic Justification |
|-----------|---------|----------------------|
| `src/data/` | Data loading & preprocessing | Separates data concerns from model logic |
| `src/models/` | Neural network architectures | Allows easy model comparison |
| `src/xai/` | Explainability methods | Isolates XAI implementation for clarity |
| `src/rag/` | Knowledge retrieval | Novel contribution - bridges XAI to explanations |
| `src/utils/` | Common utilities | Reduces code duplication |
| `notebooks/` | Experiments | Interactive development and visualization |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INPUT: CT SCAN IMAGE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PREPROCESSING (224x224, Normalize)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DEEP LEARNING MODEL (Pretrained, Fine-tuned)            â”‚
â”‚                                                                       â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚    â”‚   Features   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Prediction  â”‚        â”‚
â”‚    â”‚   (layer4)   â”‚                          â”‚   (4 class)  â”‚        â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                                         â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                         â”‚
            â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      GRAD-CAM        â”‚                    â”‚  CLASS PREDICTION    â”‚
â”‚    (Visual XAI)      â”‚                    â”‚  + Confidence Score  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                         â”‚
            â–¼                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  XAI â†’ TEXT BRIDGE   â”‚                              â”‚
â”‚  "peripheral opacity"â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
            â”‚                                         â”‚
            â–¼                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  KNOWLEDGE RETRIEVAL â”‚                              â”‚
â”‚   (Medical Facts)    â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
            â”‚                                         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FINAL OUTPUT                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Prediction  â”‚  â”‚  Grad-CAM   â”‚  â”‚      RAG Explanation        â”‚  â”‚
â”‚  â”‚Adenocarcinomaâ”‚  â”‚  Heatmap    â”‚  â”‚ "Ground-glass opacity..."  â”‚  â”‚
â”‚  â”‚   (92%)     â”‚  â”‚             â”‚  â”‚                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Installation

```bash
# Clone the repository
git clone <repository-url>
cd "Major Project"

# Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ“Š Dataset

**Source**: Kaggle - CT Scan Images of Lung Cancer Patients

**Structure**: Place the dataset in the `dataset/` folder with the following structure:
```
dataset/
â”œâ”€â”€ adenocarcinoma/
â”œâ”€â”€ squamous_cell_carcinoma/
â”œâ”€â”€ large_cell_carcinoma/
â””â”€â”€ normal/
```

**Download**: [Kaggle Dataset Link](https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images)

---

## ğŸ® Usage

### Training All Models

```bash
# Train all models (ResNet-50, MobileNetV2, ViT, Swin Transformer)
# Uses caching - already trained models are skipped automatically
python train_all_models.py

# Force retrain all models
python train_all_models.py --force-retrain

# Train specific models only
python train_all_models.py --models resnet50 mobilenetv2
```

### Model Comparison

```bash
# Compare all trained models
python compare_models.py

# This generates:
# - results/comparison/model_comparison_charts.png
# - results/comparison/model_comparison_radar.png
# - results/comparison/confusion_matrices_comparison.png
# - results/comparison/model_comparison_report.md
```

### Demo with Model Selection

```bash
# Demo with default model (ResNet-50)
python demo_multi_model.py

# Demo with specific model
python demo_multi_model.py --model mobilenetv2
python demo_multi_model.py --model vit_b_16
python demo_multi_model.py --model swin_t

# Compare all models on same image
python demo_multi_model.py --compare

# List available models and training status
python demo_multi_model.py --list
```

### Visual Demo

```bash
# Run visual demo with Grad-CAM visualization
python demo.py
python demo.py path/to/your/image.png
```

### Legacy Commands

```bash
# Train single model (legacy)
python main.py --mode train --epochs 10

# Evaluate on test set
python main.py --mode evaluate --checkpoint checkpoints/best_model_resnet50.pth

# Predict single image
python main.py --mode predict --image path/to/ct_scan.png
```

---

## ğŸ“Š Model Comparison

| Model | Parameters | Test Acc | Description | Best For |
|-------|-----------|----------|-------------|----------|
| ResNet-50 | ~25.6M | 96.97% | Deep residual network with skip connections | Default choice, excellent Grad-CAM visualizations |
| MobileNetV2 | ~3.5M | 97.40% | Lightweight network with inverted residuals | Deployment, edge devices, mobile apps |
| ViT-B/16 | ~86M | 93.51% | Attention-based transformer architecture | Research, capturing global image features |
| **Swin-T** | ~28M | **97.84%** | Hierarchical transformer with shifted windows | **Best accuracy**, production deployment |

---

## ğŸ“ˆ Results

*Results after training all models on the Lung Cancer CT Scan Dataset*

| Model | Test Accuracy | Precision | Recall | F1 Score | Training Time |
|-------|---------------|-----------|--------|----------|---------------|
| ResNet-50 | 96.97% | 96.99% | 96.97% | 96.95% | ~7 min |
| **MobileNetV2** | **97.40%** | **97.50%** | **97.40%** | **97.40%** | ~17 min |
| ViT-B/16 | 93.51% | 93.74% | 93.51% | 93.48% | ~80 min |
| Swin-T | 97.84% | 97.86% | 97.84% | 97.84% | ~28 min |

### ğŸ† Best Model: **Swin Transformer (Tiny)** with 97.84% accuracy

**Key Findings:**
- **Swin-T** achieved the highest test accuracy (97.84%) with excellent precision and recall
- **MobileNetV2** offers the best accuracy-to-efficiency ratio with only 3.5M parameters
- **ResNet-50** provides reliable performance with excellent Grad-CAM visualizations
- **ViT-B/16** requires more data/training time but captures global features well

---

## âš ï¸ Limitations

1. **Dataset Size**: Limited medical imaging data may affect generalization
2. **Grad-CAM**: Shows correlation, not causation; may highlight spurious features
3. **RAG Simplicity**: Keyword matching is basic; doesn't capture semantic meaning
4. **Clinical Validation**: Not validated by medical professionals

---

## ğŸ”® Future Enhancements

1. **Semantic RAG**: Upgrade to sentence transformers for better retrieval
2. **Multiple XAI Methods**: Add LIME, SHAP for comparison
3. **Larger Dataset**: Include more diverse CT scan sources
4. **Clinical Validation**: Partner with radiologists for validation
5. **Web Interface**: Build a user-friendly web application

---

## ğŸ“š References

1. Selvaraju, R. R., et al. (2017). "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization." ICCV 2017.
2. He, K., et al. (2016). "Deep Residual Learning for Image Recognition." CVPR 2016.
3. Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." NeurIPS 2020.

---

## ğŸ‘¥ Authors

Major Project Team - Final Year B.Tech

---

## ğŸ“„ License

This project is for academic purposes only. 
